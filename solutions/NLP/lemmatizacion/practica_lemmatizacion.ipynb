{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fe3e07",
   "metadata": {},
   "source": [
    "# Práctica: Lematización para Mejora de Análisis de Sentimiento en Reseñas Multilingües\n",
    "\n",
    "\n",
    "Implementar un pipeline de lematización que unifique variantes morfológicas y mejore la precisión de un modelo de análisis de sentimiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de7e6aa",
   "metadata": {},
   "source": [
    "## Fase 1: Diagnóstico de Problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reviews_multilang.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12884b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_counts = vectorizer.fit_transform(df[\"review_español\"])  # suponiendo columna en español\n",
    "token_freq = pd.DataFrame({'token': vectorizer.get_feature_names_out(), 'freq': X_counts.sum(axis=0).A1})\n",
    "token_freq.sort_values(by=\"freq\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de05ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Los usuarios reportaron fallas constantes: no funciona, se traba y no responde.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \" \".join(df[\"review_español\"])\n",
    "doc = nlp(text)\n",
    "lemmas = \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "wordcloud = WordCloud(background_color='white').generate(lemmas)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variantes = [(\"fallas\", \"falla\"), (\"funciona\", \"funcionar\"), (\"trabó\", \"trabar\"), (\"responder\", \"responder\"), (\"reportaron\", \"reportar\")]\n",
    "pd.DataFrame(variantes, columns=[\"Forma Original\", \"Lema\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88a3b2",
   "metadata": {},
   "source": [
    "## Fase 2: Implementación del Lematizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lematizar(texto, idioma):\n",
    "    if idioma == \"es\":\n",
    "        doc = spacy.load(\"es_core_news_sm\")(texto)\n",
    "        return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    elif idioma == \"en\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = word_tokenize(texto)\n",
    "        tagged = pos_tag(tokens)\n",
    "        return [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged if word.lower() not in stopwords.words(\"english\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef26210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(lematizar(\"Los dispositivos fallaron constantemente, no funcionan bien.\", \"es\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602ac30",
   "metadata": {},
   "source": [
    "## Fase 3: Optimización y Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759afee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df[\"review_lematizado\"] = df[\"review_español\"].apply(lambda x: \" \".join(lematizar(x, \"es\")))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_lem = vectorizer.fit_transform(df[\"review_lematizado\"])\n",
    "X_raw = TfidfVectorizer().fit_transform(df[\"review_español\"])\n",
    "y = df[\"sentimiento\"]  # asumir que existe esta columna\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lem, y, test_size=0.2)\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "print(\"F1-score con lematización:\", f1_score(y_test, preds, average=\"macro\"))\n",
    "\n",
    "X_train2, X_test2, _, _ = train_test_split(X_raw, y, test_size=0.2)\n",
    "clf2 = RandomForestClassifier().fit(X_train2, y_train)\n",
    "preds2 = clf2.predict(X_test2)\n",
    "print(\"F1-score sin lematización:\", f1_score(y_test, preds2, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1348726b",
   "metadata": {},
   "source": [
    "## Fase 4: Evaluación Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size_raw = len(TfidfVectorizer().fit(df[\"review_español\"]).vocabulary_)\n",
    "vocab_size_lem = len(TfidfVectorizer().fit(df[\"review_lematizado\"]).vocabulary_)\n",
    "print(\"Reducción de vocabulario:\", round((vocab_size_raw - vocab_size_lem) / vocab_size_raw * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578edb07",
   "metadata": {},
   "source": [
    "## Entrega Final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a58479",
   "metadata": {},
   "source": [
    "La función `lematizar(texto, idioma)` ya está implementada arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc3d78",
   "metadata": {},
   "source": [
    "\n",
    "### Requiere:\n",
    "- Comparativo de métricas pre/post lematización.\n",
    "- Ejemplos de errores corregidos como: \"trabó\" → \"trabar\", \"funcionó\" → \"funcionar\".\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

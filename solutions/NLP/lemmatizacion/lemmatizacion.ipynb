{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a30439",
   "metadata": {},
   "source": [
    "# Lematizaci√≥n para Mejora de An√°lisis de Sentimiento en Rese√±as Multiling√ºes\n",
    "Implementar un pipeline de lematizaci√≥n que unifique variantes morfol√≥gicas (\"corriendo\" ‚Üí \"correr\"), mejorando la precisi√≥n del modelo de clasificaci√≥n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafefb0b",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Diagn√≥stico de Problemas\n",
    "Objetivo: Identificar palabras no lematizadas que causan ruido en el an√°lisis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d344a7",
   "metadata": {},
   "source": [
    "### 1.1 - An√°lisis Exploratorio\n",
    "Descargar el dataset reviews_multilang.csv\n",
    "Ejemplo Cr√≠tico (Espa√±ol):\n",
    "\"Los usuarios reportaron fallas constantes: no funciona, se traba y no responde.\"  \n",
    "Procesar sin Lematizaci√≥n usando CountVectorizer:\n",
    "Contar frecuencias de tokens: [\"funciona\", \"funcionar\", \"trab√≥\", \"responder\"].\n",
    "Identificar:\n",
    "Variantes morfol√≥gicas que inflan el vocabulario.\n",
    "Errores de POS tagging (ej: \"reportaron\" etiquetado como sustantivo).\n",
    "Pistas:\n",
    "Usar spacy.load(\"es_core_news_sm\") para inspeccionar POS tags.\n",
    "Generar una nube de palabras con WordCloud para visualizar repeticiones innecesarias.\n",
    "Verificaci√≥n:\n",
    "Tabla con 5 pares de palabras que deben unificarse (ej: \"fallas\" ‚Üí \"falla\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea7f64",
   "metadata": {},
   "source": [
    "### 1.2: Implementaci√≥n del Lematizador\n",
    "Objetivo: Crear una funci√≥n que lematice texto seg√∫n su idioma y categor√≠a gramatical.\n",
    "\n",
    "Tarea 2 - Pipeline de Lematizaci√≥n\n",
    "Para Espa√±ol:\n",
    "Usar spaCy para lematizar y obtener POS tags.\n",
    "Mapear verbos a infinitivo (\"reportaron\" ‚Üí \"reportar\"), sustantivos a singular (\"fallas\" ‚Üí \"falla\").\n",
    "Para Ingl√©s:\n",
    "Usar WordNetLemmatizer de NLTK con POS tags (ej: pos='v' para verbos).\n",
    "Manejar Ambiguidades:\n",
    "En \"El banco financiero cerr√≥\", \"cerr√≥\" ‚Üí \"cerrar\" (verbo) vs \"banco\" ‚Üí \"banco\" (sustantivo).\n",
    "Requisitos:\n",
    "Input: \"Los dispositivos fallaron constantemente, no funcionan bien.\"\n",
    "Output Esperado: [\"el\", \"dispositivo\", \"fallar\", \"constantemente\", \"no\", \"funcionar\", \"bien\"].\n",
    "Pista:\n",
    "Filtrar stopwords despu√©s de lematizar.\n",
    "Usar token.lemma_ en spaCy y lemmatizer.lemmatize(token, pos) en NLTK.\n",
    "Fase 3: Optimizaci√≥n y Validaci√≥n\n",
    "Objetivo: Ajustar el lematizador para manejar jerga t√©cnica y evaluar su impacto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e029f2",
   "metadata": {},
   "source": [
    "\n",
    "Tarea 3 - Personalizaci√≥n y Pruebas (Opcional)\n",
    "M√©tricas de Rendimiento:\n",
    "Entrenar un modelo de RandomForest con y sin lematizaci√≥n. Para predecir el sentimiento.\n",
    "Comparar F1-score y tama√±o del vocabulario.\n",
    "Pista:\n",
    "Usar PhraseMatcher de spaCy para detectar t√©rminos t√©cnicos no lematizados.\n",
    "Para \"crashe√≥\", aplicar una regla regex si el lematizador no lo resuelve.\n",
    "Fase 4: Evaluaci√≥n Comparativa\n",
    "Objetivo: Medir la mejora en la precisi√≥n del modelo y reducir falsos negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcec67",
   "metadata": {},
   "source": [
    "\n",
    "Tarea 4 - An√°lisis Cuantitativo\n",
    "Dataset de Validaci√≥n:\n",
    "200 rese√±as etiquetadas manualmente (50% negativas, 50% positivas).\n",
    "Resultados Esperados:\n",
    "Reducci√≥n de vocabulario ‚â•30%.\n",
    "Aumento de F1-score ‚â•10% en rese√±as con negaciones (\"no funciona\" vs \"funcionando\").\n",
    "Pista:\n",
    "Usar TfidfVectorizer para ponderar t√©rminos clave post-lematizaci√≥n.\n",
    "Si el F1-score baja, revisar lemas de palabras negativas (\"nunca\" ‚Üí \"nunca\").\n",
    "Entrega Final\n",
    "C√≥digo:\n",
    "Funci√≥n lematizar(texto, idioma) que maneje espa√±ol e ingl√©s.\n",
    "Documentaci√≥n:\n",
    "Reporte PDF con:\n",
    "Comparativo de m√©tricas pre/post lematizaci√≥n.\n",
    "Ejemplos de errores corregidos (ej: \"trab√≥\" ‚Üí \"trabar\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48f129",
   "metadata": {},
   "source": [
    "üîç Fase 1: Diagn√≥stico de Problemas\n",
    "Tarea 1: An√°lisis Exploratorio\n",
    "1.1. Cargar Datos\n",
    "python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reviews_multilang.csv\")\n",
    "df.head()\n",
    "1.2. Procesar sin Lematizaci√≥n\n",
    "python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_counts = vectorizer.fit_transform(df[\"review_espa√±ol\"])  # suponiendo columna en espa√±ol\n",
    "token_freq = pd.DataFrame({'token': vectorizer.get_feature_names_out(), 'freq': X_counts.sum(axis=0).A1})\n",
    "token_freq.sort_values(by=\"freq\", ascending=False).head(10)\n",
    "1.3. POS Tagging con spaCy\n",
    "python\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Los usuarios reportaron fallas constantes: no funciona, se traba y no responde.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)\n",
    "1.4. Visualizaci√≥n: Nube de Palabras\n",
    "python\n",
    "text = \" \".join(df[\"review_espa√±ol\"])\n",
    "doc = nlp(text)\n",
    "lemmas = \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "wordcloud = WordCloud(background_color='white').generate(lemmas)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "1.5. Tabla de Variantes a Unificar\n",
    "python\n",
    "# Manualmente o con l√≥gica heur√≠stica\n",
    "variantes = [(\"fallas\", \"falla\"), (\"funciona\", \"funcionar\"), (\"trab√≥\", \"trabar\"), (\"responder\", \"responder\"), (\"reportaron\", \"reportar\")]\n",
    "pd.DataFrame(variantes, columns=[\"Forma Original\", \"Lema\"])\n",
    "üõ†Ô∏è Fase 2: Implementaci√≥n del Lematizador\n",
    "Tarea 2: Pipeline de Lematizaci√≥n\n",
    "python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # fallback\n",
    "\n",
    "def lematizar(texto, idioma):\n",
    "    if idioma == \"es\":\n",
    "        doc = spacy.load(\"es_core_news_sm\")(texto)\n",
    "        return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    elif idioma == \"en\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = word_tokenize(texto)\n",
    "        tagged = pos_tag(tokens)\n",
    "        return [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged if word.lower() not in stopwords.words(\"english\")]\n",
    "Prueba:\n",
    "python\n",
    "print(lematizar(\"Los dispositivos fallaron constantemente, no funcionan bien.\", \"es\"))\n",
    "‚öôÔ∏è Fase 3: Optimizaci√≥n y Validaci√≥n\n",
    "Tarea 3: M√©tricas de Rendimiento\n",
    "python\n",
    "# Entrenar RandomForest con y sin lematizaci√≥n\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocesamiento\n",
    "df[\"review_lematizado\"] = df[\"review_espa√±ol\"].apply(lambda x: \" \".join(lematizar(x, \"es\")))\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_lem = vectorizer.fit_transform(df[\"review_lematizado\"])\n",
    "X_raw = TfidfVectorizer().fit_transform(df[\"review_espa√±ol\"])\n",
    "y = df[\"sentimiento\"]  # asumimos que existe esta columna\n",
    "\n",
    "# Modelo con lematizaci√≥n\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lem, y, test_size=0.2)\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "print(\"F1-score con lematizaci√≥n:\", f1_score(y_test, preds, average=\"macro\"))\n",
    "\n",
    "# Sin lematizaci√≥n\n",
    "X_train2, X_test2, _, _ = train_test_split(X_raw, y, test_size=0.2)\n",
    "clf2 = RandomForestClassifier().fit(X_train2, y_train)\n",
    "preds2 = clf2.predict(X_test2)\n",
    "print(\"F1-score sin lematizaci√≥n:\", f1_score(y_test, preds2, average=\"macro\"))\n",
    "üìä Fase 4: Evaluaci√≥n Comparativa\n",
    "Tarea 4: An√°lisis Cuantitativo\n",
    "python\n",
    "vocab_size_raw = len(TfidfVectorizer().fit(df[\"review_espa√±ol\"]).vocabulary_)\n",
    "vocab_size_lem = len(TfidfVectorizer().fit(df[\"review_lematizado\"]).vocabulary_)\n",
    "print(\"Reducci√≥n de vocabulario:\", round((vocab_size_raw - vocab_size_lem) / vocab_size_raw * 100, 2), \"%\")\n",
    "üìå Entrega Final\n",
    "Funci√≥n Final:\n",
    "python\n",
    "# Ya definida como `lematizar(texto, idioma)`\n",
    "Documentaci√≥n Esperada:\n",
    "Comparativo de m√©tricas pre/post lematizaci√≥n.\n",
    "\n",
    "Ejemplos como: \"trab√≥\" ‚Üí \"trabar\", \"funcion√≥\" ‚Üí \"funcionar\".\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af082bc",
   "metadata": {},
   "source": [
    "\n",
    "# Práctica: Normalización de Texto para Análisis Multilingüe en Reseñas de Productos\n",
    "\n",
    "**Contexto:** Eres parte del equipo de NLP de **GlobalReviews**, una empresa que procesa reseñas de usuarios en español, francés e inglés para detectar defectos en productos electrónicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba130d",
   "metadata": {},
   "source": [
    "\n",
    "## Fase 1: Diagnóstico de Problemas\n",
    "\n",
    "**Objetivo:** Identificar errores de codificación, acentos inconsistentes y contracciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14bfce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv(\"resenas_multilingue.csv\")\n",
    "print(df.sample(5))  # Muestra algunos ejemplos\n",
    "\n",
    "# Ejemplo crítico\n",
    "ejemplo = \"Ã‰ste celular es increÃ­ble!!! Pero tarda 5hs en cargarse... #BaterÃ­aMala 😞\"\n",
    "print(ejemplo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2024a9b",
   "metadata": {},
   "source": [
    "\n",
    "## Fase 2: Normalización Unicode y Codificación\n",
    "\n",
    "**Objetivo:** Corregir textos corruptos y unificar formatos Unicode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import chardet\n",
    "import unicodedata\n",
    "\n",
    "# Detección de codificación con chardet\n",
    "with open(\"resenas_multilingue.csv\", \"rb\") as f:\n",
    "    print(chardet.detect(f.read(10000)))\n",
    "\n",
    "# Ejemplo de corrección de codificación\n",
    "def corregir_codificacion(texto):\n",
    "    try:\n",
    "        return texto.encode('latin1').decode('utf8')\n",
    "    except:\n",
    "        return texto\n",
    "\n",
    "# Normalización Unicode\n",
    "def normalizar_unicode(texto):\n",
    "    return unicodedata.normalize('NFC', texto)\n",
    "\n",
    "texto_corregido = corregir_codificacion(ejemplo)\n",
    "texto_normalizado = normalizar_unicode(texto_corregido)\n",
    "print(texto_normalizado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e38463",
   "metadata": {},
   "source": [
    "\n",
    "## Fase 3: Manejo de Acentos y Contracciones\n",
    "\n",
    "**Objetivo:** Eliminar acentos opcionales y expandir contracciones coloquiales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Diccionario de contracciones\n",
    "contracciones = {\n",
    "    \"q'huvo\": \"que hubo\",\n",
    "    \"pq\": \"porque\",\n",
    "    \" x \": \" por \"\n",
    "}\n",
    "\n",
    "# Normalización de hashtags\n",
    "def normalizar_hashtag(hashtag):\n",
    "    return \"#\" + unidecode(hashtag[1:]).lower()\n",
    "\n",
    "# Pipeline completo de normalización\n",
    "def normalizar_texto(texto):\n",
    "    texto = corregir_codificacion(texto)\n",
    "    texto = normalizar_unicode(texto)\n",
    "    texto = texto.lower()\n",
    "    for contra, exp in contracciones.items():\n",
    "        texto = texto.replace(contra, exp)\n",
    "    texto = re.sub(r'#\\w+', lambda m: normalizar_hashtag(m.group()), texto)\n",
    "    texto = unidecode(texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    return texto.strip()\n",
    "\n",
    "print(normalizar_texto(\"Ã‰ste es un buen móvil!!! No q'huvo fallos. #Duradero❤️\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f4338",
   "metadata": {},
   "source": [
    "\n",
    "## Fase 4: Evaluación de Impacto\n",
    "\n",
    "**Objetivo:** Medir cómo la normalización afecta la calidad del análisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b99f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supongamos que df tiene columna 'texto' y 'etiqueta'\n",
    "df['texto_normalizado'] = df['texto'].apply(normalizar_texto)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['texto'], df['etiqueta'], test_size=0.3, random_state=42)\n",
    "X_train_norm, X_test_norm = train_test_split(df['texto_normalizado'], test_size=0.3, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "Xv_train = vectorizer.fit_transform(X_train)\n",
    "Xv_test = vectorizer.transform(X_test)\n",
    "\n",
    "Xv_train_norm = vectorizer.fit_transform(X_train_norm)\n",
    "Xv_test_norm = vectorizer.transform(X_test_norm)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xv_train, y_train)\n",
    "acc_orig = accuracy_score(y_test, model.predict(Xv_test))\n",
    "\n",
    "model.fit(Xv_train_norm, y_train)\n",
    "acc_norm = accuracy_score(y_test, model.predict(Xv_test_norm))\n",
    "\n",
    "print(\"Accuracy sin normalización:\", acc_orig)\n",
    "print(\"Accuracy con normalización:\", acc_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17fa38",
   "metadata": {},
   "source": [
    "\n",
    "## Entrega Final\n",
    "\n",
    "1. Script: `normalizacion.py`\n",
    "2. Diccionario contracciones: `contracciones.txt`\n",
    "3. Reporte PDF comparativo\n",
    "4. Casos de test: `test_normalizacion.json`\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
